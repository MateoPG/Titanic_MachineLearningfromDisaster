{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_set(data = 'train'):\n",
    "    \"\"\"_summary_\n",
    "    This function reads the csv that contains the training data or test data for the project\n",
    "    Args:\n",
    "        data (str, optional): _description_. Defaults to 'train'.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    return pd.read_csv(f'../Data/{data}.csv')\n",
    "\n",
    "def find_closest_cabin(fare_value, means_dataset):\n",
    "    \"\"\"_summary_\n",
    "    This function finds the closest class in a class column using the mean value of other given column (fare in this case) and a given Fare value to compare.\n",
    "    Args:\n",
    "        fare_value (_type_): _description_\n",
    "        means_dataset (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    closest_letter = min(means_dataset['Fare'].keys(), key=lambda x: abs(means_dataset['Fare'][x] - fare_value))\n",
    "    return closest_letter\n",
    "\n",
    "def imput_age_by_pclass_and_sibsp(row_sibps_and_pclass, grouped_ages):\n",
    "    \"\"\"_summary_\n",
    "    This function finds the mean/median or other metric value for the age grouped by SibSp and Pclass and imputs it into a DataFrame given a SibSp and a Pclass\n",
    "    Args:\n",
    "        row_sibps_and_pclass (_type_): _description_\n",
    "        grouped_ages (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    age_to_imput = grouped_ages[(grouped_ages['SibSp']== row_sibps_and_pclass['SibSp'])\n",
    "                                &(grouped_ages['Pclass']== row_sibps_and_pclass['Pclass'])]['Age'].values[0]\n",
    "    if not age_to_imput > 0:\n",
    "        age_to_imput = grouped_ages['Age'].mean()\n",
    "    return age_to_imput\n",
    "\n",
    "def imput_fare_by_pclass(row_pclass, grouped_fares):\n",
    "    \"\"\"_summary_\n",
    "    This function finds the mean/median or other metric value for the Fare grouped by Pclass and imputs it into a DataFrame given a Pclass\n",
    "    Args:\n",
    "        row_pclass (_type_): _description_\n",
    "        grouped_fares (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    age_to_imput = grouped_fares[(grouped_fares['Pclass']== row_pclass['Pclass'])]['Fare'].values[0]\n",
    "    if not age_to_imput > 0:\n",
    "        age_to_imput = grouped_fares['Fare'].mean()\n",
    "    return age_to_imput\n",
    "\n",
    "def feature_imputation(database):\n",
    "    \"\"\"_summary_\n",
    "    \n",
    "    This function unifies the imputation process for the features Cabin, Embarked and Age.\n",
    "    \n",
    "    Args:\n",
    "        database (_type_): _description_\n",
    "    \"\"\"\n",
    "    database['cabin_letter'] = database.apply(lambda x: x['Cabin'] if pd.isnull(x['Cabin']) else str(x['Cabin'])[0], axis = 1)\n",
    "\n",
    "    mean_fares_by_cabin = database[database['Cabin'].notnull()][['Fare','cabin_letter']].groupby('cabin_letter').mean().sort_values(by = 'Fare')\n",
    "    median_fares_by_cabin = database[database['Cabin'].notnull()][['Fare','cabin_letter']].groupby('cabin_letter').median().sort_values(by = 'Fare')\n",
    "    database['imputed_cabin_letter_by_mean'] = database['cabin_letter'].fillna(database['Fare'].apply(lambda x: find_closest_cabin(x, mean_fares_by_cabin)))\n",
    "    database['imputed_cabin_letter_by_median'] = database['cabin_letter'].fillna(database['Fare'].apply(lambda x: find_closest_cabin(x, median_fares_by_cabin)))\n",
    "\n",
    "    mean_fares_by_port = database[database['Cabin'].notnull()][['Fare','Embarked']].groupby('Embarked').mean().sort_values(by = 'Fare')\n",
    "    median_fares_by_port = database[database['Cabin'].notnull()][['Fare','Embarked']].groupby('Embarked').median().sort_values(by = 'Fare')\n",
    "    database['imputed_Embarked_by_mean'] = database['Embarked'].fillna(database['Fare'].apply(lambda x: find_closest_cabin(x, mean_fares_by_port)))\n",
    "    database['imputed_Embarked_by_median'] = database['Embarked'].fillna(database['Fare'].apply(lambda x: find_closest_cabin(x, median_fares_by_port)))\n",
    "\n",
    "    mean_ages_grouped = database[['Age', 'SibSp', 'Pclass']].groupby(['SibSp', 'Pclass']).mean().reset_index()\n",
    "    median_ages_grouped = database[['Age', 'SibSp', 'Pclass']].groupby(['SibSp', 'Pclass']).median().reset_index()\n",
    "    database['imputed_Age_by_mean'] = database['Age'].fillna(database.apply(lambda x: imput_age_by_pclass_and_sibsp(x, mean_ages_grouped), axis = 1))\n",
    "    database['imputed_Age_by_median'] = database['Age'].fillna(database.apply(lambda x: imput_age_by_pclass_and_sibsp(x, median_ages_grouped), axis = 1))\n",
    "    \n",
    "    mean_fares_grouped = database[['Fare', 'Pclass']].groupby(['Pclass']).mean().reset_index()\n",
    "    median_fares_grouped = database[['Fare', 'Pclass']].groupby(['Pclass']).median().reset_index()\n",
    "    database['imputed_Fare_by_mean'] = database['Fare'].fillna(database.apply(lambda x: imput_fare_by_pclass(x, mean_fares_grouped), axis = 1))\n",
    "    database['imputed_Fare_by_median'] = database['Fare'].fillna(database.apply(lambda x: imput_fare_by_pclass(x, median_fares_grouped), axis = 1))\n",
    "    \n",
    "    return database\n",
    "\n",
    "def feature_one_hot_encoding(database, columns_to_process):\n",
    "    \"\"\"_summary_\n",
    "    This function creates multiple columns based on the categorical variables listed in the\n",
    "    column_to_process parameter, where each column splits in n columns, having n as the \n",
    "    number of categories in the respective column.\n",
    "    Args:\n",
    "        database (_type_): _description_\n",
    "        columns_to_process (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    one_hot_db = pd.DataFrame()\n",
    "\n",
    "    for column in columns_to_process:\n",
    "        temp_one_hot_db = pd.get_dummies(database[column], prefix= column + '_')\n",
    "        one_hot_db = pd.concat([one_hot_db, temp_one_hot_db], axis = 1)\n",
    "        \n",
    "    database = pd.concat([database, one_hot_db], axis = 1)\n",
    "    \n",
    "    return database\n",
    "\n",
    "def feature_creation(database):\n",
    "    \"\"\"_summary_\n",
    "    This function creates new features using the existent features (original or imputed), combining them,\n",
    "    separating them or doing other processes.\n",
    "    Args:\n",
    "        database (_type_): _description_\n",
    "    \"\"\"\n",
    "    database['surname'] = database['Name'].apply(lambda x: x.split(',')[0])\n",
    "    #database['age_in_months'] = database['Age']*12\n",
    "    database['imputed_age_in_months_by_mean'] = database['imputed_Age_by_mean']*12\n",
    "    database['imputed_age_in_months_by_median'] = database['imputed_Age_by_median']*12\n",
    "    database['family_members'] = database['SibSp'] + database['Parch']\n",
    "    \n",
    "    return database\n",
    "\n",
    "def feature_scalation(database, columns_to_scale_and_transform):\n",
    "    \"\"\"_summary_\n",
    "    This function scales the numerical features given in the columns_to_scale_and_transform list. It also\n",
    "    gives the result of the column applying a logaritmit transformation\n",
    "    Args:\n",
    "        database (_type_): _description_\n",
    "        columns_to_scale_and_transform (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    variables_result = {}\n",
    "    for column in columns_to_scale_and_transform:\n",
    "        variables_result['scaler_for_' + column] = StandardScaler().fit(database[column].to_numpy().reshape(-1, 1))\n",
    "        variables_result['scaled_' + column] = variables_result['scaler_for_' + column].transform(database[column].to_numpy().reshape(-1, 1))\n",
    "        variables_result['log_' + column] = np.log(database[column])\n",
    "        \n",
    "        database = pd.concat([database, pd.DataFrame(variables_result['scaled_' + column], columns = ['scaled_' + column])], axis = 1)\n",
    "        database = pd.concat([database, variables_result['log_' + column].rename(\"log_\" + column)], axis = 1)\n",
    "\n",
    "    return database\n",
    "\n",
    "def feature_reduction(database, dimensions_to_reduce):\n",
    "    \n",
    "    pca = PCA(n_components = 2)\n",
    "    pca.fit(database[dimensions_to_reduce])\n",
    "    reduced_dimensions = pd.DataFrame(pca.transform(database[dimensions_to_reduce]), columns = ['First_component', 'Second_component'])\n",
    "    \n",
    "    return reduced_dimensions\n",
    "\n",
    "def base_consolidation(database, reduced_dimensions, columns_to_discard_in_train, dimensions_to_reduce):\n",
    "\n",
    "    X_train_set = database[[col for col in database.columns if col not in columns_to_discard_in_train + dimensions_to_reduce]]\n",
    "    X_train_set = pd.concat([X_train_set, reduced_dimensions], axis = 1)\n",
    "    try:\n",
    "        Y_train_set = X_train_set.pop('Survived')\n",
    "    except:\n",
    "        Y_train_set = None\n",
    "    return X_train_set, Y_train_set\n",
    "\n",
    "def model_training(X_train_set, Y_train_set):\n",
    "    \n",
    "    logistic_regression = LogisticRegression(random_state=0, max_iter = 1000).fit(X_train_set, Y_train_set)\n",
    "    logistic_score = logistic_regression.score(X_train_set, Y_train_set)\n",
    "\n",
    "    random_forest = RandomForestClassifier(n_estimators=10, random_state=0).fit(X_train_set, Y_train_set)\n",
    "    random_forest_score = random_forest.score(X_train_set, Y_train_set)\n",
    "\n",
    "    gradient_boosting = GradientBoostingClassifier(n_estimators=100000, learning_rate=0.1,\n",
    "                        max_depth=1, random_state=0).fit(X_train_set, Y_train_set)\n",
    "    gradient_boosting_score = gradient_boosting.score(X_train_set, Y_train_set)\n",
    "    \n",
    "    return logistic_regression, logistic_score, random_forest, random_forest_score, gradient_boosting, gradient_boosting_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train_set = load_data_set()\n",
    "original_test_set = load_data_set(data = 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The percentage of nulls in the columns tell us that Age, Cabin and Embarked could be inferred. The Cabin could be difficult, but as it could be highly related with the Fare, something could be made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train_set.isnull().sum() / len(original_train_set) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boxplots between some variables and Age, with the aim of finding relations to imput Age data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=3, cols=2,\n",
    "    specs=[[{}, {}],\n",
    "           [{}, {}],\n",
    "           [{\"colspan\": 2}, None]],\n",
    "    subplot_titles=(\"Sex\", \"Embarked\", \"Pclass\", \"SibSp\", \"Parch\")\n",
    "    )\n",
    "fig.add_trace(go.Box(x=original_train_set[\"Sex\"], y=original_train_set[\"Age\"], name=\"Age by Sex\", boxpoints=\"all\"), row = 1, col = 1)\n",
    "fig.add_trace(go.Box(x=original_train_set[\"Embarked\"], y=original_train_set[\"Age\"], name=\"Age by Port\", boxpoints=\"all\"), row = 1, col = 2)\n",
    "fig.add_trace(go.Box(x=original_train_set[\"Pclass\"], y=original_train_set[\"Age\"], name=\"Age by Pclass\", boxpoints=\"all\"), row = 2, col = 1)\n",
    "fig.add_trace(go.Box(x=original_train_set[\"SibSp\"], y=original_train_set[\"Age\"], name=\"Age by SibSp\", boxpoints=\"all\"), row = 2, col = 2)\n",
    "fig.add_trace(go.Box(x=original_train_set[\"Parch\"], y=original_train_set[\"Age\"], name=\"Age by Parch\", boxpoints=\"all\"), row = 3, col = 1)\n",
    "fig.update_layout(height=700, showlegend=False, title_text=\"Age by different variables\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations between Age and other variables to find relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train_set[original_train_set.dtypes[(original_train_set.dtypes =='int64')|(original_train_set.dtypes =='float64')].index].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.imshow(original_train_set[original_train_set.dtypes[(original_train_set.dtypes =='int64')|(original_train_set.dtypes =='float64')].index].corr())\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given SibSp and Pclass the more related variables to Age, some boxplots are represented including the 3 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(original_train_set[original_train_set['Age'].isna()], y=\"SibSp\", points=\"all\", color = 'Pclass')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(original_train_set[original_train_set['SibSp']==0], y=\"Age\", points=\"all\", color = 'Pclass')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boxplot of Embarked and Fare to see relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(original_train_set, x=\"Embarked\", y=\"Fare\", points=\"all\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(original_train_set[original_train_set['Embarked'].isna()], y=\"Fare\", points=\"all\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying The ideas from the dataset to create or fill columns:\n",
    "* Surnames could be extract - But they're too many, so not used until necessary\n",
    "* Age could be tourned into months.\n",
    "* The cabins maybe related to Tickets, Fare or others.\n",
    "* The port of embarcation could be inferred?\n",
    "* The age could be imputed using SibSp and Pclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = copy.deepcopy(original_train_set)\n",
    "train_set['surname'] = train_set['Name'].apply(lambda x: x.split(',')[0])\n",
    "train_set['cabin_letter'] = train_set.apply(lambda x: x['Cabin'] if pd.isnull(x['Cabin']) else str(x['Cabin'])[0], axis = 1)\n",
    "mean_fares_by_cabin = train_set[train_set['Cabin'].notnull()][['Fare','cabin_letter']].groupby('cabin_letter').mean().sort_values(by = 'Fare')\n",
    "median_fares_by_cabin = train_set[train_set['Cabin'].notnull()][['Fare','cabin_letter']].groupby('cabin_letter').median().sort_values(by = 'Fare')\n",
    "train_set['imputed_cabin_letter_by_mean'] = train_set['cabin_letter'].fillna(train_set['Fare'].apply(lambda x: find_closest_cabin(x, mean_fares_by_cabin)))\n",
    "train_set['imputed_cabin_letter_by_median'] = train_set['cabin_letter'].fillna(train_set['Fare'].apply(lambda x: find_closest_cabin(x, median_fares_by_cabin)))\n",
    "mean_fares_by_port = train_set[train_set['Cabin'].notnull()][['Fare','Embarked']].groupby('Embarked').mean().sort_values(by = 'Fare')\n",
    "median_fares_by_port = train_set[train_set['Cabin'].notnull()][['Fare','Embarked']].groupby('Embarked').median().sort_values(by = 'Fare')\n",
    "train_set['imputed_Embarked_by_mean'] = train_set['Embarked'].fillna(train_set['Fare'].apply(lambda x: find_closest_cabin(x, mean_fares_by_port)))\n",
    "train_set['imputed_Embarked_by_median'] = train_set['Embarked'].fillna(train_set['Fare'].apply(lambda x: find_closest_cabin(x, median_fares_by_port)))\n",
    "mean_ages_grouped = train_set[['Age', 'SibSp', 'Pclass']].groupby(['SibSp', 'Pclass']).mean().reset_index()\n",
    "median_ages_grouped = train_set[['Age', 'SibSp', 'Pclass']].groupby(['SibSp', 'Pclass']).median().reset_index()\n",
    "train_set['imputed_Age_by_mean'] = train_set['Age'].fillna(train_set.apply(lambda x: imput_age_by_pclass_and_sibsp(x, mean_ages_grouped), axis = 1))\n",
    "train_set['imputed_Age_by_median'] = train_set['Age'].fillna(train_set.apply(lambda x: imput_age_by_pclass_and_sibsp(x, median_ages_grouped), axis = 1))\n",
    "mean_fares_grouped = train_set[['Fare', 'Pclass']].groupby(['Pclass']).mean().reset_index()\n",
    "median_fares_grouped = train_set[['Fare', 'Pclass']].groupby(['Pclass']).median().reset_index()\n",
    "train_set['imputed_Fare_by_mean'] = train_set['Fare'].fillna(train_set.apply(lambda x: imput_fare_by_pclass(x, mean_fares_grouped), axis = 1))\n",
    "train_set['imputed_Fare_by_median'] = train_set['Fare'].fillna(train_set.apply(lambda x: imput_fare_by_pclass(x, median_fares_grouped), axis = 1))\n",
    "train_set['age_in_months'] = train_set['Age']*12\n",
    "train_set['imputed_age_in_months_by_mean'] = train_set['imputed_Age_by_mean']*12\n",
    "train_set['imputed_age_in_months_by_median'] = train_set['imputed_Age_by_median']*12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boxplot of cabin_letter and Fare to see relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(train_set, x=\"cabin_letter\", y=\"Fare\", points=\"all\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relations between survived and numercial columns\n",
    "Pclass, Sibsp, Parch and Fare seems to have relation with the Survived categorization, while Age just in the extreme cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows = int(len(train_set.dtypes[(train_set.dtypes =='int64')|(train_set.dtypes =='float64')])/2) - 1, cols=2,\n",
    "    subplot_titles = [\"Survived by \"+col for col in train_set.dtypes[(train_set.dtypes =='int64')|(train_set.dtypes =='float64')].index if col not in ['PassengerId', 'Survived']])\n",
    "height_per_row = 200\n",
    "for idx, column in enumerate(train_set.dtypes[(train_set.dtypes =='int64')|(train_set.dtypes =='float64')].index):\n",
    "    if column not in ['PassengerId', 'Survived']: \n",
    "        fig.add_trace(go.Box(x=train_set[\"Survived\"], y=train_set[column], boxpoints=\"all\"), row = math.floor(idx/2), col = idx%2 + 1)\n",
    "fig.update_layout(height=height_per_row*(math.floor(idx/2) + 1), showlegend=False, title_text=\"Survived relations with numerical features\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relations between survived and non numercial columns\n",
    "Sex, Embarked, cabin_letter and its imputations seems to have relation with Survived categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows = math.ceil(len(train_set.dtypes[(train_set.dtypes =='object')].index)/2)-2, cols=2,\n",
    "    subplot_titles = [\"Survived by \"+ col for col in train_set.dtypes[(train_set.dtypes =='object')].index if col not in ['Name', 'Ticket', 'surname', 'Cabin']])\n",
    "height_per_row = 200\n",
    "idx = 0\n",
    "for column in train_set.dtypes[(train_set.dtypes =='object')].index:\n",
    "    grouped_train_set = train_set[['PassengerId', 'Survived', column]].groupby(['Survived', column]).count().reset_index()\n",
    "    if column not in ['Name', 'Ticket', 'surname', 'Cabin']:\n",
    "        survived_class = grouped_train_set['Survived'].unique()\n",
    "        fig.add_trace(go.Bar(x=grouped_train_set[grouped_train_set['Survived']==survived_class[0]][column],\n",
    "                             y=grouped_train_set[grouped_train_set['Survived']==survived_class[0]]['PassengerId'], \n",
    "                             name='Not Survived',\n",
    "                             marker_color='Red',  \n",
    "                             legendgroup = idx+1), row = math.floor(idx/2)+1, col = idx%2 + 1)\n",
    "        fig.add_trace(go.Bar(x=grouped_train_set[grouped_train_set['Survived']==survived_class[1]][column],\n",
    "                             y=grouped_train_set[grouped_train_set['Survived']==survived_class[1]]['PassengerId'],\n",
    "                             name='Survived', \n",
    "                             marker_color='Blue',\n",
    "                             legendgroup = idx+1), row = math.floor(idx/2)+1, col = idx%2 + 1)\n",
    "        idx += 1\n",
    "fig.update_layout(barmode='group', height=height_per_row*(math.floor(idx/2) + 1), title_text=\"Survived relations with categorical features\", legend_tracegroupgap = height_per_row*(math.floor(idx/2) + 1))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical variables description after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding statistical differences between numerical variables\n",
    "The only numerical variable that seems to have a difference between its mean by Survived category is Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_columns_to_compare = ['Fare', 'imputed_Age_by_mean', 'imputed_Age_by_median', 'imputed_Fare_by_mean',\t'imputed_Fare_by_median', 'imputed_age_in_months_by_mean', 'imputed_age_in_months_by_median']\n",
    "for column_name in list_of_columns_to_compare:\n",
    "    test_result = ttest_ind(train_set[train_set['Survived'] == 1][column_name], train_set[train_set['Survived'] == 0][column_name])\n",
    "    if test_result.pvalue < 0.05:\n",
    "        conclusion = f\"there is difference for the mean {column_name} between those who survived and those who did not in the Titanic tragedy\"\n",
    "    else:\n",
    "        conclusion = f\"there is NO difference for the mean {column_name} between those who survived and those who did not in the Titanic tragedy\"\n",
    "        \n",
    "    print(f\"\"\"The t-test two-sided test for the variable {column_name} to find differences between the survivors gave the following results:\n",
    "           statistic: {test_result.statistic:.2f}\n",
    "           pvalue: {test_result.pvalue:.4f}\n",
    "           Given those results, the test allows to conclude that {conclusion}\n",
    "           \"\"\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new features throught feature engineering\n",
    "### One hot encoding\n",
    "As in general, classification models do not understand categories as one variable, a one hot encoding is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_onehot = ['Pclass', 'Sex', 'imputed_cabin_letter_by_mean', 'imputed_cabin_letter_by_median', 'imputed_Embarked_by_mean', 'imputed_Embarked_by_median']\n",
    "one_hot_db = pd.DataFrame()\n",
    "\n",
    "for column in columns_to_onehot:\n",
    "    temp_one_hot_db = pd.get_dummies(train_set[column], prefix= column + '_')\n",
    "    one_hot_db = pd.concat([one_hot_db, temp_one_hot_db], axis = 1)\n",
    "    \n",
    "train_set = pd.concat([train_set, one_hot_db], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining features\n",
    "In order to create new features, the Sipsp and Parch (like horizontal and vertical movements in the family tree) are unified to know the total family close persons in the Titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['family_members'] = train_set['SibSp'] + train_set['Parch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and transforming columns with outliers\n",
    "In order to improve the performance of the model, numerical variables are scaled to make them more comparable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_scale_and_transform = [\"Fare\", \"imputed_Age_by_mean\", \"imputed_Age_by_median\", \"imputed_Fare_by_mean\",\t\"imputed_Fare_by_median\", \"age_in_months\", \"imputed_age_in_months_by_mean\", \"imputed_age_in_months_by_median\"]\n",
    "variables_result = {}\n",
    "for column in columns_to_scale_and_transform:\n",
    "    variables_result['scaler_for_' + column] = StandardScaler().fit(train_set[column].to_numpy().reshape(-1, 1))\n",
    "    variables_result['scaled_' + column] = variables_result['scaler_for_' + column].transform(train_set[column].to_numpy().reshape(-1, 1))\n",
    "    variables_result['log_' + column] = np.log(train_set[column])\n",
    "    \n",
    "    train_set = pd.concat([train_set, pd.DataFrame(variables_result['scaled_' + column], columns = ['scaled_' + column])], axis = 1)\n",
    "    train_set = pd.concat([train_set, variables_result['log_' + column].rename(\"log_\" + column)], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA - Dimensionality reduction\n",
    "After the EDA process, many numerical variables where created, each with different adjustments in their imputations and scalings, so a PCA is conducted in order to reduce the dimensionality created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions_to_reduce = ['scaled_imputed_Age_by_mean', 'log_imputed_Age_by_mean',\n",
    "                        'scaled_imputed_Age_by_median', 'log_imputed_Age_by_median', 'scaled_imputed_Fare_by_mean',\n",
    "                        'scaled_imputed_Fare_by_median', 'scaled_imputed_age_in_months_by_mean',\n",
    "                        'log_imputed_age_in_months_by_mean', 'scaled_imputed_age_in_months_by_median',\n",
    "                        'log_imputed_age_in_months_by_median']\n",
    "pca = PCA(n_components = 2)\n",
    "pca.fit(train_set[dimensions_to_reduce])\n",
    "reduced_dimensions = pd.DataFrame(pca.transform(train_set[dimensions_to_reduce]), columns = ['First_component', 'Second_component'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=go.Scatter(x=reduced_dimensions['First_component'], y=reduced_dimensions['Second_component'], mode='markers', marker_color = train_set['Survived']))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the previous graph, it seems that the components don't predict as well the Survivor behaviour, but they help to reduce the number of numerical variables in the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test NaN validation\n",
    "As the train set could have different characteristics than the test set, a validation of NaN for the inputation process is made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_test_set.isnull().sum() / len(original_test_set) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx%2 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows = int(len(original_test_set.dtypes[(original_test_set.dtypes =='int64')|(original_test_set.dtypes =='float64')])/2 -1), cols=2,\n",
    "    subplot_titles = [\"Fare by \"+col for col in original_test_set.dtypes[(original_test_set.dtypes =='int64')|(original_test_set.dtypes =='float64')].index if col not in ['PassengerId', 'Fare']])\n",
    "height_per_row = 200\n",
    "for idx, column in enumerate([col for col in original_test_set.dtypes[(original_test_set.dtypes =='int64')|(original_test_set.dtypes =='float64')].index if col not in ['PassengerId', 'Fare']]):\n",
    "    fig.add_trace(go.Box(x=original_test_set[\"Fare\"], y=original_test_set[column], boxpoints=\"all\"), row = math.floor(idx/2) + 1, col = idx%2 + 1)\n",
    "fig.update_layout(height=height_per_row*(math.floor(idx/2) + 1), showlegend=False, title_text=\"Fare relations with numerical features\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection\n",
    "First of all, a feature selection of the database is made, in order to select the features to use. Discarding the original features, or the imputed but not encoded, scaled or reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_discard_in_train = ['PassengerId', 'Fare', 'Pclass','Name', 'Sex', 'Age', 'Ticket', 'Cabin', 'Embarked',\n",
    "    'surname', 'cabin_letter', 'imputed_cabin_letter_by_mean', 'scaled_Fare', 'imputed_cabin_letter_by_median',\n",
    "    'imputed_Embarked_by_mean', 'imputed_Embarked_by_median', 'imputed_Age_by_mean', 'imputed_Age_by_median',\n",
    "    'age_in_months', 'imputed_age_in_months_by_mean', 'imputed_age_in_months_by_median', 'log_Fare', \n",
    "    'log_imputed_Fare_by_mean', 'log_imputed_Fare_by_median', 'scaled_age_in_months', 'log_age_in_months']\n",
    "\n",
    "X_train_set = train_set[[col for col in train_set.columns if col not in columns_to_discard_in_train + dimensions_to_reduce]]\n",
    "X_train_set = pd.concat([X_train_set, reduced_dimensions], axis = 1)\n",
    "\n",
    "Y_train_set = X_train_set.pop('Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = LogisticRegression(random_state=0, max_iter = 1000).fit(X_train_set, Y_train_set)\n",
    "logistic_score = logistic_regression.score(X_train_set, Y_train_set)\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators = 10, random_state=0).fit(X_train_set, Y_train_set)\n",
    "random_forest_score = random_forest.score(X_train_set, Y_train_set)\n",
    "\n",
    "gradient_boosting = GradientBoostingClassifier(n_estimators = 100000, learning_rate=0.1,\n",
    "                    max_depth=1, random_state=0).fit(X_train_set, Y_train_set)\n",
    "gradient_boosting_score = gradient_boosting.score(X_train_set, Y_train_set)\n",
    "\n",
    "print(logistic_score, random_forest_score, gradient_boosting_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = feature_imputation(original_train_set)\n",
    "columns_to_onehot = ['Pclass', 'Sex', 'imputed_cabin_letter_by_mean', 'imputed_cabin_letter_by_median', 'imputed_Embarked_by_mean', 'imputed_Embarked_by_median']\n",
    "train_set = feature_one_hot_encoding(train_set, columns_to_onehot)\n",
    "train_set = feature_creation(train_set)\n",
    "columns_to_scale_and_transform = [\"Fare\", \"imputed_Age_by_mean\", \"imputed_Age_by_median\", \"imputed_Fare_by_mean\", \"imputed_Fare_by_median\", \"imputed_age_in_months_by_mean\", \"imputed_age_in_months_by_median\"]\n",
    "train_set = feature_scalation(train_set, columns_to_scale_and_transform)\n",
    "dimensions_to_reduce = ['scaled_imputed_Age_by_mean', 'log_imputed_Age_by_mean',\n",
    "                        'scaled_imputed_Age_by_median', 'log_imputed_Age_by_median', 'scaled_imputed_Fare_by_mean',\n",
    "                        'scaled_imputed_Fare_by_median', 'scaled_imputed_age_in_months_by_mean',\n",
    "                        'log_imputed_age_in_months_by_mean', 'scaled_imputed_age_in_months_by_median',\n",
    "                        'log_imputed_age_in_months_by_median']\n",
    "reduced_dimensions = feature_reduction(train_set, dimensions_to_reduce)\n",
    "columns_to_discard_in_train = ['PassengerId', 'Fare', 'Pclass','Name', 'Sex', 'Age', 'Ticket', 'Cabin', 'Embarked',\n",
    "    'surname', 'cabin_letter', 'imputed_cabin_letter_by_mean', 'scaled_Fare', 'imputed_cabin_letter_by_median',\n",
    "    'imputed_Embarked_by_mean', 'imputed_Embarked_by_median', 'imputed_Age_by_mean', 'imputed_Age_by_median',\n",
    "    'age_in_months', 'imputed_age_in_months_by_mean', 'imputed_age_in_months_by_median', 'log_Fare', \n",
    "    'log_imputed_Fare_by_mean', 'log_imputed_Fare_by_median', 'scaled_age_in_months', 'log_age_in_months']\n",
    "X_train_set, Y_train_set = base_consolidation(train_set, reduced_dimensions, columns_to_discard_in_train, dimensions_to_reduce)\n",
    "logistic_regression, logistic_score, random_forest, random_forest_score, gradient_boosting, gradient_boosting_score = model_training(X_train_set, Y_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = feature_imputation(original_test_set)\n",
    "columns_to_onehot = ['Pclass', 'Sex', 'imputed_cabin_letter_by_mean', 'imputed_cabin_letter_by_median', 'imputed_Embarked_by_mean', 'imputed_Embarked_by_median']\n",
    "test_set = feature_one_hot_encoding(test_set, columns_to_onehot)\n",
    "test_set = feature_creation(test_set)\n",
    "columns_to_scale_and_transform = [\"Fare\", \"imputed_Age_by_mean\", \"imputed_Age_by_median\", \"imputed_Fare_by_mean\", \"imputed_Fare_by_median\", \"imputed_age_in_months_by_mean\", \"imputed_age_in_months_by_median\"]\n",
    "test_set = feature_scalation(test_set, columns_to_scale_and_transform)\n",
    "dimensions_to_reduce = ['scaled_imputed_Age_by_mean', 'log_imputed_Age_by_mean',\n",
    "                        'scaled_imputed_Age_by_median', 'log_imputed_Age_by_median', 'scaled_imputed_Fare_by_mean',\n",
    "                        'scaled_imputed_Fare_by_median', 'scaled_imputed_age_in_months_by_mean',\n",
    "                        'log_imputed_age_in_months_by_mean', 'scaled_imputed_age_in_months_by_median',\n",
    "                        'log_imputed_age_in_months_by_median']\n",
    "reduced_dimensions = feature_reduction(test_set, dimensions_to_reduce)\n",
    "columns_to_discard_in_test = ['PassengerId', 'Fare', 'Pclass','Name', 'Sex', 'Age', 'Ticket', 'Cabin', 'Embarked',\n",
    "    'surname', 'cabin_letter', 'imputed_cabin_letter_by_mean', 'scaled_Fare', 'imputed_cabin_letter_by_median',\n",
    "    'imputed_Embarked_by_mean', 'imputed_Embarked_by_median', 'imputed_Age_by_mean', 'imputed_Age_by_median',\n",
    "    'age_in_months', 'imputed_age_in_months_by_mean', 'imputed_age_in_months_by_median', 'log_Fare', \n",
    "    'log_imputed_Fare_by_mean', 'log_imputed_Fare_by_median', 'scaled_age_in_months', 'log_age_in_months']\n",
    "X_test_set, _ = base_consolidation(test_set, reduced_dimensions, columns_to_discard_in_test, dimensions_to_reduce)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
